# TÃœRK ÃœRÃœN ARAMA ASISTANI - RAG (Retrieval-Augmented Generation) UYGULAMASI

# Temel Python kÃ¼tÃ¼phaneleri
import os
import re
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from datasets import load_dataset

# Haystack AI framework - RAG pipeline iÃ§in temel bileÅŸenler
from haystack import Pipeline
from haystack.dataclasses import Document 
from haystack.document_stores.in_memory import InMemoryDocumentStore 
from haystack.components.preprocessors import DocumentSplitter 
from haystack.components.writers.document_writer import DocumentWriter 

# Embedding bileÅŸenleri - metinleri vektÃ¶re Ã§evirmek iÃ§in
from haystack.components.embedders.sentence_transformers_document_embedder import SentenceTransformersDocumentEmbedder  # DokÃ¼manlarÄ± vektÃ¶re Ã§evirir
from haystack.components.embedders.sentence_transformers_text_embedder import SentenceTransformersTextEmbedder  # KullanÄ±cÄ± sorgularÄ±nÄ± vektÃ¶re Ã§evirir
from haystack.utils import Secret
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever 

# Chat ve AI generation bileÅŸenleri
from haystack.components.builders.chat_prompt_builder import ChatPromptBuilder
from haystack.dataclasses import ChatMessage
from haystack_integrations.components.generators.google_genai import GoogleGenAIChatGenerator

# Google Gemini API anahtarÄ± ve Hugging Face token'Ä± yÃ¼kleniyor
try:
    load_dotenv()  # .env dosyasÄ±ndan deÄŸiÅŸkenleri yÃ¼kle
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")  # Google Gemini iÃ§in gerekli
    HF_TOKEN = os.getenv("HF_TOKEN")              # Hugging Face iÃ§in opsiyonel
    if not GOOGLE_API_KEY:
        st.error("GOOGLE_API_KEY bulunamadÄ±.")
        st.stop()
except Exception as e:
    st.error(f"Ortam deÄŸiÅŸkenleri yÃ¼klenirken bir hata oluÅŸtu: {e}")
    st.stop()

# VERÄ° YÃœKLEME VE HAZIRLIK FONKSÄ°YONU

@st.cache_resource  # Streamlit cache - bir kez yÃ¼kleyip hafÄ±zada tut
def load_and_prepare_data():
    """
    Hugging Face'ten TÃ¼rk gÄ±da Ã¼rÃ¼nleri veri setini yÃ¼kler,
    kolonlarÄ± sabit eÅŸler, Document'lere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve parÃ§alara ayÄ±rÄ±r.
    """
    with st.spinner("ÃœrÃ¼n kataloÄŸu veri seti yÃ¼kleniyor..."):
        try:
            # Hugging Face'ten veri setini yÃ¼kle
            ds = load_dataset("Hulusiaa/tr_food_product_catalog_with_ingredients", split="train")
            df = ds.to_pandas()

            if df.empty:
                st.error("Veri seti boÅŸ gÃ¶rÃ¼nÃ¼yor.")
                return None

            # SÃ¼tun eÅŸlemeleri - dataset kolonlarÄ±nÄ± tanÄ±mla
            COL_NAME       = "product_name"                           # ÃœrÃ¼n adÄ±
            COL_BRAND      = "brand"                                  # Marka
            COL_CATEGORY   = "category"                               # Kategori
            COL_PRICE      = "price"                                  # Fiyat
            COL_INGR_LIST  = "ingredientsList"                       # Ä°Ã§indekiler listesi
            COL_INGR_ALT   = "Ingredients"                           # Alternatif iÃ§indekiler
            COL_ORIGIN     = "Origin"                                # MenÅŸei
            COL_WEIGHT     = "Net Weight (g/ml)"                     # Net aÄŸÄ±rlÄ±k
            COL_USAGE      = "Usage Recommendations"                  # KullanÄ±m Ã¶nerileri
            COL_ALLERGEN   = "Allergen Warning"                      # Alerjen uyarÄ±sÄ±
            COL_STORAGE    = "Storage Conditions"                    # Saklama koÅŸullarÄ±
            COL_PRODUCER   = "GÄ±da Ä°ÅŸletmecisi / Ãœretici / Ä°thalatÃ§Ä± / DaÄŸÄ±tÄ±cÄ±"  # Ãœretici bilgisi

            if COL_NAME not in df.columns:
                st.error(f"Gerekli sÃ¼tun bulunamadÄ±: {COL_NAME}")
                return None

            # Veri temizliÄŸi - boÅŸ ve geÃ§ersiz kayÄ±tlarÄ± kaldÄ±r
            df_clean = df.dropna(subset=[COL_NAME]).copy()
            df_clean = df_clean[df_clean[COL_NAME].astype(str).str.strip() != ""]
            df_clean.reset_index(drop=True, inplace=True)

            # Haystack Document objelerine dÃ¶nÃ¼ÅŸtÃ¼rme
            documents = []
            for _, row in df_clean.iterrows():
                # GÃ¼venli deÄŸer alma fonksiyonu
                def val(col):
                    return str(row[col]).strip() if (col in df_clean.columns and pd.notna(row[col])) else ""

                # TÃ¼m Ã¼rÃ¼n bilgilerini Ã§Ä±kar
                name      = val(COL_NAME)
                brand     = val(COL_BRAND)
                category  = val(COL_CATEGORY)
                price     = val(COL_PRICE)
                ingr_list = val(COL_INGR_LIST)
                ingr_alt  = val(COL_INGR_ALT)
                origin    = val(COL_ORIGIN)
                weight    = val(COL_WEIGHT)
                usage     = val(COL_USAGE)
                allergen  = val(COL_ALLERGEN)
                storage   = val(COL_STORAGE)
                producer  = val(COL_PRODUCER)

                # AI'Ä±n okuyacaÄŸÄ± text formatÄ±nÄ± oluÅŸtur
                content_parts = [f"ÃœrÃ¼n AdÄ±: {name}"]  # Her zaman Ã¼rÃ¼n adÄ± ile baÅŸla
                if category:  content_parts.append(f"Kategori: {category}")
                if brand:     content_parts.append(f"Marka: {brand}")
                if price:     content_parts.append(f"Fiyat: {price}")
                if weight:    content_parts.append(f"Net Miktar: {weight}")
                if ingr_list: content_parts.append(f"Ä°Ã§indekiler (detay): {ingr_list}")
                elif ingr_alt: content_parts.append(f"Ä°Ã§indekiler: {ingr_alt}")
                if usage:     content_parts.append(f"KullanÄ±m Ã–nerileri: {usage}")
                if storage:   content_parts.append(f"Saklama KoÅŸullarÄ±: {storage}")
                if allergen:  content_parts.append(f"Alerjen UyarÄ±sÄ±: {allergen}")
                if origin:    content_parts.append(f"MenÅŸei: {origin}")
                if producer:  content_parts.append(f"Ä°ÅŸletmeci/Ãœretici/Ä°thalatÃ§Ä±/DaÄŸÄ±tÄ±cÄ±: {producer}")

                content = "\n\n".join(content_parts)

                # Metadata - hÄ±zlÄ± filtreleme iÃ§in key bilgiler
                meta = {
                    "name": name,
                    "brand": brand,
                    "category": category,
                    "price": price,
                    "weight": weight,
                    "origin": origin,
                }

                # Haystack Document objesi oluÅŸtur
                documents.append(Document(content=content, meta=meta))

            if not documents:
                st.error("Veri setinden belge Ã¼retilemedi.")
                return None

            # Document splitting (chunking) - bÃ¼yÃ¼k dokÃ¼manlarÄ± kÃ¼Ã§Ã¼k parÃ§alara bÃ¶l
            # Bu, AI modellerin token limitlerini aÅŸmamak ve daha iyi retrieval iÃ§in Ã¶nemli
            splitter = DocumentSplitter(split_by="word", split_length=220, split_overlap=40)
            split_docs = splitter.run(documents)
            return split_docs["documents"]

        except Exception as e:
            st.error(f"ÃœrÃ¼n veri seti hazÄ±rlanÄ±rken hata: {e}")
            return None

# VEKTÃ–R VERÄ°TABANI OLUÅTURMA FONKSÄ°YONU

@st.cache_resource  # Bir kez oluÅŸtur, hafÄ±zada tut
def create_faiss_index(_split_docs):
    """
    Verilen belgeler iÃ§in bir InMemory DocumentStore oluÅŸturur ve doldurur.
    Bu fonksiyon dokÃ¼manlarÄ± vektÃ¶rlere Ã§evirip aranabilir hale getirir.
    """
    if not _split_docs:
        return None
        
    with st.spinner("VektÃ¶r veritabanÄ± oluÅŸturuluyor ve belgeler iÅŸleniyor..."):
        try:
            # RAM'de Ã§alÄ±ÅŸan hÄ±zlÄ± document store
            document_store = InMemoryDocumentStore()
            
            # TÃ¼rkÃ§e metinler iÃ§in optimize edilmiÅŸ embedding modeli
            doc_embedder = SentenceTransformersDocumentEmbedder(
                model="trmteb/turkish-embedding-model"  # TÃ¼rkÃ§e'ye Ã¶zel model
            )

            # Belgeleri ve gÃ¶mme vektÃ¶rlerini deposuna yazmak iÃ§in bir boru hattÄ±
            # Pipeline: dokÃ¼manlarÄ± al -> vektÃ¶re Ã§evir -> veritabanÄ±na yaz
            indexing_pipeline = Pipeline()
            indexing_pipeline.add_component("embedder", doc_embedder)
            indexing_pipeline.add_component("writer", DocumentWriter(document_store=document_store))
            indexing_pipeline.connect("embedder.documents", "writer.documents")

            # Boru hattÄ±nÄ± Ã§alÄ±ÅŸtÄ±rarak indeksi oluÅŸtur
            indexing_pipeline.run({"embedder": {"documents": _split_docs}})
            
            return document_store
        except Exception as e:
            st.error(f"VektÃ¶r indeksi oluÅŸturulurken hata oluÅŸtu: {e}")
            return None

# RAG PIPELINE OLUÅTURMA FONKSÄ°YONU

@st.cache_resource  # Bir kez oluÅŸtur, hafÄ±zada tut
def build_rag_pipeline(_document_store):
    """
    RAG (Retrieval-Augmented Generation) Pipeline oluÅŸturur.
    Bu pipeline: kullanÄ±cÄ± sorusu -> benzer dokÃ¼manlarÄ± bul -> AI ile yanÄ±t Ã¼ret
    """
    if not _document_store:
        return None

    try:
        # KullanÄ±cÄ± sorgularÄ±nÄ± vektÃ¶re Ã§eviren embedder
        # HF_TOKEN varsa authenticated eriÅŸim, yoksa public
        if HF_TOKEN:
            text_embedder = SentenceTransformersTextEmbedder(
                model="trmteb/turkish-embedding-model",
                token=Secret.from_token(HF_TOKEN)
            )
        else:
            text_embedder = SentenceTransformersTextEmbedder(
                model="trmteb/turkish-embedding-model"
            )

        # Benzer dokÃ¼manlarÄ± bulan retriever (en benzer 6 dokÃ¼man getir)
        retriever = InMemoryEmbeddingRetriever(document_store=_document_store, top_k=6)

        # Chat iÃ§in mesaj tabanlÄ± ÅŸablon - AI'Ä±n nasÄ±l davranacaÄŸÄ±nÄ± belirler
        template = [
            # Sistem mesajÄ±: AI'Ä±n rolÃ¼ ve kurallarÄ±
            ChatMessage.from_system(
                "Sen bir Ã¼rÃ¼n katalog asistanÄ±sÄ±n. YalnÄ±zca verilen belgelerdeki bilgilere dayanarak yanÄ±t ver. "
                "Belgeler yetersizse 'Belgelerimde bu konu hakkÄ±nda yeterli bilgi bulamadÄ±m.' de."
            ),
            # KullanÄ±cÄ± mesajÄ±: dinamik olarak doldurulacak template
            ChatMessage.from_user(
                    """ÃœrÃ¼nler:
                {% for doc in documents %}
                ---
                Belge (ÃœrÃ¼n ParÃ§asÄ±):
                {{ doc.content }}
                (ÃœrÃ¼n: {{ doc.meta['name'] }} | Kategori: {{ doc.meta['category'] }} | Marka: {{ doc.meta['brand'] }} | Fiyat: {{ doc.meta['price'] }} | Net: {{ doc.meta['weight'] }} | MenÅŸei: {{ doc.meta['origin'] }})
                ---
                {% endfor %}

                Soru: {{question}}"""
                            ),
                        ]

        # Template'i dinamik verilerle dolduran prompt builder
        prompt_builder = ChatPromptBuilder(
            template=template,
            required_variables=["question"]  # Zorunlu deÄŸiÅŸken: kullanÄ±cÄ± sorusu
        )

        # Google Gemini AI ile yanÄ±t Ã¼reten generator
        generator = GoogleGenAIChatGenerator(
            model="gemini-2.0-flash",  # En son Gemini modeli
            api_key=Secret.from_token(GOOGLE_API_KEY)
        )

        # RAG Pipeline montajÄ± - tÃ¼m bileÅŸenleri birleÅŸtir
        rag_pipeline = Pipeline()
        rag_pipeline.add_component("text_embedder", text_embedder)    # 1. Sorguyu vektÃ¶re Ã§evir
        rag_pipeline.add_component("retriever", retriever)           # 2. Benzer dokÃ¼manlarÄ± bul
        rag_pipeline.add_component("prompt_builder", prompt_builder) # 3. Prompt'u hazÄ±rla
        rag_pipeline.add_component("generator", generator)           # 4. AI yanÄ±tÄ± Ã¼ret

        # BaÄŸlantÄ±lar - veri akÄ±ÅŸÄ±nÄ± tanÄ±mla
        rag_pipeline.connect("text_embedder.embedding", "retriever.query_embedding")  # Sorgu vektÃ¶rÃ¼ -> Retriever
        rag_pipeline.connect("retriever.documents", "prompt_builder.documents")       # Bulunan dokÃ¼manlar -> Prompt builder
        rag_pipeline.connect("prompt_builder.prompt", "generator.messages")           # HazÄ±rlanan prompt -> AI generator
        
        return rag_pipeline

    except Exception as e:
        st.error(f"RAG boru hattÄ± oluÅŸturulurken hata oluÅŸtu: {e}")
        return None


# ANA UYGULAMA FONKSÄ°YONU
def main():
    """
    Streamlit web uygulamasÄ±nÄ±n ana fonksiyonu.
    Sayfa ayarlarÄ±, veri yÃ¼kleme ve chat arayÃ¼zÃ¼nÃ¼ yÃ¶netir.
    """
    # Streamlit sayfa konfigÃ¼rasyonu
    st.set_page_config(page_title="ÃœrÃ¼n Arama AsistanÄ±", page_icon="ğŸ›’")

    # Ana baÅŸlÄ±k ve aÃ§Ä±klama
    st.title("ğŸ›’ TÃ¼rkiye ÃœrÃ¼n Arama AsistanÄ±")
    st.caption("ğŸ¤– **AI Destekli RAG Sistemi** | ğŸ“Š **Veri:** `Hulusiaa/tr_food_product_catalog_with_ingredients`")
    

    # Sistem baÅŸlatma sÃ¼reci - veri yÃ¼kleme ve pipeline oluÅŸturma
    split_documents = load_and_prepare_data()           # 1. Veri setini yÃ¼kle ve iÅŸle
    if split_documents:
        document_store = create_faiss_index(split_documents)  # 2. VektÃ¶r veritabanÄ±nÄ± oluÅŸtur
        rag_pipeline = build_rag_pipeline(document_store) if document_store else None  # 3. RAG pipeline'Ä± hazÄ±rla
    else:
        rag_pipeline = None

    # Sistem hazÄ±r deÄŸilse uygulamayÄ± durdur
    if not rag_pipeline:
        st.warning("Uygulama baÅŸlatÄ±lamadÄ±. LÃ¼tfen hata mesajlarÄ±nÄ± kontrol edin.")
        st.stop()

    # Chat geÃ§miÅŸi iÃ§in session state yÃ¶netimi
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Ã–nceki chat mesajlarÄ±nÄ± gÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])


    # HÄ±zlÄ± eriÅŸim butonlarÄ± - Ã¶rnek sorular iÃ§in
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ¥› Bana Laktozsuz sÃ¼t Ã¶nerir misin?", use_container_width=True):
            st.session_state.sample_question = "Bana Laktozsuz sÃ¼t Ã¶nerir misin?"
    
    with col2:
        if st.button("ï¿½ Gurmepack Pesto Fusilli 400 G fiyatÄ± ne kadar?", use_container_width=True):
            st.session_state.sample_question = "Gurmepack Pesto Fusilli 400 G fiyatÄ± ne kadar?"
    
    with col3:
        if st.button("ğŸ« Ãœlker Ã‡ikolatalÄ± Gofretin iÃ§erÄŸinde neler var listele", use_container_width=True):
            st.session_state.sample_question = "Ãœlker Ã‡ikolatalÄ± Gofretin iÃ§erÄŸinde neler var listele"
    
    # chat input alanÄ±
    placeholder = "Sorunuzu buraya yazÄ±n... (Ã–rn: Ãœlker markalÄ± biskÃ¼viler)"
    
    # Ã–rnek soru butonuna basÄ±ldÄ±ysa otomatik doldur
    default_value = st.session_state.get("sample_question", "")
    # Chat input ile soru alma
    if prompt := st.chat_input(placeholder):
        # Ã–rnek soru state'ini temizle
        if "sample_question" in st.session_state:
            del st.session_state.sample_question
            
        # KullanÄ±cÄ± mesajÄ±nÄ± kaydet ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
    
    # Ã–rnek soru butonuna basÄ±ldÄ±ysa otomatik iÅŸle
    elif "sample_question" in st.session_state and st.session_state.sample_question:
        prompt = st.session_state.sample_question
        del st.session_state.sample_question  # State'i temizle
        
        # KullanÄ±cÄ± mesajÄ±nÄ± kaydet ve gÃ¶ster
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
    
    # EÄŸer prompt varsa iÅŸleme devam et
    if 'prompt' in locals() and prompt:

        # RAG Pipeline ile yanÄ±t Ã¼retme sÃ¼reci
        with st.spinner("ğŸ” Ä°lgili Ã¼rÃ¼nler taranÄ±yor ve AI yanÄ±tÄ± hazÄ±rlanÄ±yor..."):
            try:
                # RAG Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r - en Ã¶nemli adÄ±m!
                # Bu tek satÄ±r: sorguyu vektÃ¶re Ã§evir -> benzer dokÃ¼manlarÄ± bul -> AI yanÄ±tÄ± Ã¼ret
                result = rag_pipeline.run({
                    "text_embedder": {"text": prompt},      # KullanÄ±cÄ± sorgusunu vektÃ¶re Ã§evir
                    "prompt_builder": {"question": prompt}  # Prompt template'ine soruyu ekle
                })

                # AI yanÄ±tÄ±nÄ± Ã§Ä±karma ve temizleme - gÃ¼Ã§lÃ¼ parsing
                response = "Bir hata oluÅŸtu veya yanÄ±t alÄ±namadÄ±."
                if result and "generator" in result and result["generator"].get("replies"):
                    replies = result["generator"]["replies"]
                    if isinstance(replies, list) and len(replies) > 0:
                        chat_message = replies[0]
                        
                        # ChatMessage objesinden text Ã§Ä±karma
                        try:
                            # Ä°lk olarak content attribute'unu kontrol et
                            if hasattr(chat_message, 'content') and isinstance(chat_message.content, list):
                                response = chat_message.content[0].text
                            elif hasattr(chat_message, 'content'):
                                response = str(chat_message.content)
                            else:
                                # String olarak Ã§evir ve regex ile text'i Ã§Ä±kar
                                full_str = str(chat_message)
                                # text='...' kÄ±smÄ±nÄ± bul
                                match = re.search(r"text='([^']*)'", full_str)
                                if match:
                                    response = match.group(1)
                                else:
                                    # text="..." formatÄ±
                                    match = re.search(r'text="([^"]*)"', full_str)
                                    if match:
                                        response = match.group(1)
                                    else:
                                        response = full_str
                            
                            # Escape karakterleri dÃ¼zeltir (\n, \t, \r)
                            if isinstance(response, str):
                                response = response.replace('\\n', '\n').replace('\\t', '\t').replace('\\r', '\r')
                                
                        except Exception as e:
                            # Son Ã§are: manuel parsing
                            try:
                                full_str = str(chat_message)
                                match = re.search(r"text='([^']*)'", full_str)
                                if match:
                                    response = match.group(1).replace('\\n', '\n').replace('\\t', '\t').replace('\\r', '\r')
                                else:
                                    response = "YanÄ±t alÄ±namadÄ±."
                            except:
                                response = "YanÄ±t iÅŸlenirken hata oluÅŸtu."
                    else:
                        response = str(replies)

            except Exception as e:
                response = f"Sorgu iÅŸlenirken bir hata oluÅŸtu: {e}"

        # AI yanÄ±tÄ±nÄ± kaydet ve gÃ¶ster
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)


if __name__ == "__main__":
    main()  # Ana fonksiyonu Ã§aÄŸÄ±r